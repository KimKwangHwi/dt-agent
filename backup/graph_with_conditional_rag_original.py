import os
import json
import asyncio
import httpx
import logging
import faiss
import numpy as np
from typing import Annotated, List, Dict, Any, TypedDict, Union
from pathlib import Path
from dotenv import load_dotenv

from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from pydantic import BaseModel, Field
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_anthropic import ChatAnthropic
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEmbeddings

# temp.pyì—ì„œ ìš”ì•½ ë©”ë‰´ì–¼ê³¼ ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from temp import SHORT_MANUAL, CATEGORY_DICT, SHORTER_MANUAL

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
# ê¸°ë³¸ ë¡œê¹…ì€ ë„ê³  ì»¤ìŠ¤í…€ ì¶œë ¥ë§Œ ë´…ë‹ˆë‹¤
logging.basicConfig(level=logging.CRITICAL)

# -------------------------------------------------------------------------
# 0. VectorDB ê´€ë ¨ ì„¤ì •
# -------------------------------------------------------------------------
DATA_DIR = Path(__file__).resolve().parent.parent / "data"
DB_PATH = DATA_DIR / "faiss_db.json"
DB_PERMANENT_PATH = DATA_DIR / "faiss_db_permanent.json"
INDEX_PATH = DATA_DIR / "faiss_index.bin"
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

def load_db(file_path):
    if file_path.exists():
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_db(file_path, db):
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(db, f, ensure_ascii=False, indent=4)

def load_faiss_index():
    if INDEX_PATH.exists() and INDEX_PATH.stat().st_size > 0:
        try:
            with open(INDEX_PATH, "rb") as f:
                data = f.read()
            return faiss.deserialize_index(np.frombuffer(data, dtype=np.uint8))
        except Exception as e:
            print(f"Error loading FAISS index, creating new one: {e}")
            return faiss.IndexFlatL2(768)
    return faiss.IndexFlatL2(768)

def save_faiss_index(index):
    index_data = faiss.serialize_index(index)
    with open(INDEX_PATH, "wb") as f:
        f.write(index_data)

db = load_db(DB_PATH)
db_permanent = load_db(DB_PERMANENT_PATH)
index = load_faiss_index()

# -------------------------------------------------------------------------
# 1. ë””ë²„ê¹… ë° ë¡œê¹… ìœ í‹¸ë¦¬í‹°
# -------------------------------------------------------------------------

class DebugCallbackHandler(BaseCallbackHandler):
    def __init__(self, show_prompt: bool = True, show_token: bool = True):
        self.show_prompt = show_prompt
        self.show_token = show_token

    def on_chat_model_start(self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any) -> None:
        if not self.show_prompt: return
        print("\n" + "ğŸ”µ " * 10 + " [LLM INPUT PROMPT] " + "ğŸ”µ " * 10)
        for msg in messages[0]:
            content = str(msg.content)
            if len(content) > 1500:
                display_content = content[:500] + "\n... (ì¤‘ëµ: ë§¤ë‰´ì–¼ ë³¸ë¬¸) ...\n" + content[-500:]
            else:
                display_content = content
            print(f"[{msg.type.upper()}]: {display_content}")
        print("ğŸ”µ " * 25 + "\n")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:

        if not self.show_token:
            return
        
        try:
            generation = response.generations[0][0]
            usage = {}
            if hasattr(generation, 'message') and hasattr(generation.message, 'response_metadata'):
                usage = generation.message.response_metadata.get('token_usage', {}) or generation.message.response_metadata.get('usage', {})
            elif hasattr(response, 'llm_output') and response.llm_output:
                usage = response.llm_output.get('token_usage', {})

            if usage:
                input_tokens = usage.get('input_tokens', 0) or usage.get('prompt_tokens', 0)
                output_tokens = usage.get('output_tokens', 0) or usage.get('completion_tokens', 0)
                total = usage.get('total_tokens', 0) or (input_tokens + output_tokens)
                print(f"ğŸ“Š [TOKEN USAGE] Input: {input_tokens} | Output: {output_tokens} | Total: {total}")
            else:
                print("ğŸ“Š [TOKEN USAGE] ë©”íƒ€ë°ì´í„° ì—†ìŒ")
        except Exception as e:
            print(f"ğŸ“Š [TOKEN USAGE] íŒŒì‹± ì‹¤íŒ¨: {e}")


def print_state_debug(node_name: str, state: Dict):
    print(f"\n" + "âš¡ " * 15)
    print(f"âš¡ [STATE DUMP] Node: {node_name}")
    print(f"âš¡ " * 15)
    keys_to_show = ["current_stage_index", "api_results", "final_answer", "documents"]
    if "endpoint_plan" in state and state["endpoint_plan"]:
        print(f"  - endpoint_plan: {len(state['endpoint_plan'])} stages loaded")
    if "executable_stage" in state and state["executable_stage"]:
        print(f"  - executable_stage: {len(state['executable_stage'].get('steps', []))} steps ready")
    for k in keys_to_show:
        if k in state and state[k]:
            val = state[k]
            if k == "documents":
                 print(f"  - documents: (Content loaded, length: {len(val)})")
            elif k == "api_results":
                print(f"  - api_results: {list(val.keys())}")
            else:
                print(f"  - {k}: {val}")
    print("-" * 60 + "\n")

# -------------------------------------------------------------------------
# 2. ì„¤ì • ë° ë°ì´í„° ëª¨ë¸
# -------------------------------------------------------------------------

JSON_PATH = Path(__file__).resolve().parent.parent / "backend" / "manual" / "uri_params.json"
PARAMS_JSON = load_db(JSON_PATH)

# --- ì¶œë ¥ ëª¨ë¸ ---
class ApiStep(BaseModel):
    step_id: str = Field(description="ë‹¨ê³„ ì‹ë³„ì (ì˜ˆ: '1-1', '1-2')")
    endpoint: str = Field(description="í˜¸ì¶œí•  API ì—”ë“œí¬ì¸íŠ¸")
    reasoning: str = Field(description="ì´ ë‹¨ê³„ì˜ ì‹¤í–‰ ëª©ì ")

class ParallelStage(BaseModel):
    stage_id: int = Field(description="ì‹¤í–‰ ìˆœì„œ (1ë¶€í„° ì‹œì‘)")
    steps: List[ApiStep] = Field(description="ì´ ë‹¨ê³„ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ì—”ë“œí¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸")

class EndpointExecutionPlan(BaseModel):
    plan: List[ParallelStage] = Field(description="ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë  ìŠ¤í…Œì´ì§€ ë¦¬ìŠ¤íŠ¸")

class ExecutableStep(BaseModel):
    step_id: str = Field(description="ê³ ìœ  ì‹ë³„ì (ì˜ˆ: '3-1-instance-1')")
    endpoint: str = Field(description="í˜¸ì¶œí•  API ì—”ë“œí¬ì¸íŠ¸")
    params: Dict[str, Any] = Field(description="API í˜¸ì¶œì— ì‚¬ìš©í•  ì‹¤ì œ íŒŒë¼ë¯¸í„°")
    reasoning: str = Field(description="ì´ íŠ¹ì • API í˜¸ì¶œì„ ì‹¤í–‰í•˜ëŠ” ì´ìœ ")

class ExecutableStage(BaseModel):
    steps: List[ExecutableStep] = Field(description="ì´ë²ˆ ìŠ¤í…Œì´ì§€ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  API í˜¸ì¶œ ëª©ë¡")

# --- ì¹´í…Œê³ ë¦¬ ì„ íƒ ëª¨ë¸ (ìƒˆë¡œ ì¶”ê°€) ---
class CategorySelection(BaseModel):
    categories: List[str] = Field(description="ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì¹´í…Œê³ ë¦¬ ì´ë¦„ ëª©ë¡")


# --- ë©”ì¸ Agent State (ìˆ˜ì •) ---
class AgentState(TypedDict):
    question: str
    documents: str  # RAG ì‹¤íŒ¨ ì‹œ API ëª…ì„¸ì„œë¥¼ ë‹´ì„ í•„ë“œ
    endpoint_plan: List[Dict] 
    executable_stage: Dict 
    current_stage_index: int
    api_results: Dict
    final_answer: str
    from_db: bool # DBì—ì„œ ì™”ëŠ”ì§€ ì—¬ë¶€

# -------------------------------------------------------------------------
# 3. ë„êµ¬(Tool) ë° ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
# -------------------------------------------------------------------------

async def call_torus_api(endpoint: str, params: Dict[str, Any] = {}) -> Dict:
    base_url = "http://127.0.0.1:8000"
    url = f"{base_url}{endpoint}"
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, params=params, timeout=10.0)
            if response.status_code == 200:
                print(f"   ğŸ“¥ [API Response] Success from {url} | Params: {params}")
                return response.json()
            else:
                print(f"   âš ï¸ [API Response] Failed from {url} | Status: {response.status_code}")
                return {"error": f"Status {response.status_code}", "body": response.text}
    except Exception as e:
        print(f"   ğŸ”¥ [API Response] Exception from {url} | Error: {e}")
        return {"error": str(e)}

class TorusDynamicAgent:
    def __init__(self, model):
        self.debug_handler = DebugCallbackHandler()
        self.model = model
        self.category_selector_model = model.with_structured_output(CategorySelection)
        self.endpoint_planner_model = model.with_structured_output(EndpointExecutionPlan)
        self.param_planner_model = model.with_structured_output(ExecutableStage)
        self.PARAMS_JSON = PARAMS_JSON

    async def get_params_info(self, endpoint_list: List[str]):
        results = {}
        for endpoint in endpoint_list:
            endpoint_info = self.PARAMS_JSON.get(endpoint)
            results[endpoint] = endpoint_info.get("required_params") if endpoint_info else "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì—”ë“œí¬ì¸íŠ¸"
        return results
    
    # --- ë…¸ë“œ ì •ì˜ ---
    
    async def vector_store_node(self, state: AgentState):
        print_state_debug("Vector Store", state)
        question = state["question"]
        
        if index.ntotal > 0:
            query_embedding = np.array([embedding_model.embed_query(question)], dtype='float32')
            distances, indices = index.search(query_embedding, 1)
            
            print(f"ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ : {distances[0][0]}")
            if distances[0][0] < 10:
                matched_question = list(db.keys())[indices[0][0]]
                print(f"ğŸ” Found similar question in DB: '{matched_question}'")
                return {
                    "endpoint_plan": db[matched_question],
                    "from_db": True, "documents": "",
                    "current_stage_index": 0, "api_results": {}
                }

        print("ğŸš« No similar question found in DB. Proceeding to Category Selection.")
        return {"from_db": False, "documents": ""}

    async def category_selection_node(self, state: AgentState):
        print_state_debug("Category Selection (Temp Node)", state)
        question = state["question"]
        
        prompt = (
            "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬, ì§ˆë¬¸ í•´ê²°ì— í•„ìš”í•œ API ì¹´í…Œê³ ë¦¬ë¥¼ ì •í™•íˆ ì‹ë³„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
            "ì£¼ì–´ì§„ ì§ˆë¬¸ì˜ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì•„ë˜ JSON í˜•ì‹ì˜ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì—ì„œ **ê´€ë ¨ëœ ëª¨ë“  ì¹´í…Œê³ ë¦¬**ë¥¼ ì‹ ì¤‘í•˜ê²Œ ì„ íƒí•˜ì„¸ìš”.\n\n"
            f"### ì‚¬ìš©ì ì§ˆë¬¸:\n{question}\n\n"
            f"### ì „ì²´ ì¹´í…Œê³ ë¦¬ ëª©ë¡ (JSON í˜•ì‹):\n{SHORT_MANUAL}\n\n"
            "ë¶„ì„ í›„, ê´€ë ¨ëœ ì¹´í…Œê³ ë¦¬ë“¤ì˜ `category` í•„ë“œ ê°’ë§Œ JSON ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”."
        )
        
        selection_obj = await self.category_selector_model.ainvoke(
            [HumanMessage(content=prompt)],
            config={"callbacks": [self.debug_handler]}
        )
        
        selected_categories = selection_obj.categories
        print(f"\nğŸ§  LLM selected categories: {selected_categories}")
        
        # ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ API ëª…ì„¸ì„œë¥¼ ìˆ˜ì§‘
        relevant_docs = []
        for cat in selected_categories:
            if cat in CATEGORY_DICT:
                relevant_docs.append(f"--- Category: {cat} ---\n{CATEGORY_DICT[cat]}")
        
        documents_str = "\n\n".join(relevant_docs)
        print("ğŸ“š Assembled relevant API documents for the planner.")
        
        return {"documents": documents_str}

    async def endpoint_planner_node(self, state: AgentState):
        print_state_debug("Endpoint Planner", state)
        question = state["question"]
        documents = state["documents"] # RAG ì‹¤íŒ¨ ì‹œ category_selection_nodeê°€ ì±„ì›Œì¤Œ

        if not documents: # ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ ë°©ì§€ìš© (ì´ë¡ ìƒ ì—¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì•ˆë¨)
            return {"endpoint_plan": [], "current_stage_index": 0, "api_results": {}}
            
        system_text = (
            "ë‹¹ì‹ ì€ TORUS API ë³‘ë ¬ ì²˜ë¦¬ ì„¤ê³„ìì…ë‹ˆë‹¤. **ì£¼ì–´ì§„ API ëª…ì„¸ì„œë§Œì„ ì°¸ê³ í•˜ì—¬** ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤."
            "ì„œë¡œ ì˜ì¡´ì„±ì´ ì—†ëŠ” API í˜¸ì¶œì€ ê°™ì€ ìŠ¤í…Œì´ì§€(Stage)ë¡œ ë¬¶ì–´ì•¼ í•©ë‹ˆë‹¤.\n\n"
            "=== ì‘ì„± ê·œì¹™ ===\n"
            "0. **machine list**: ëª¨ë“  ì§ˆë¬¸ì€ /machine/list (í•„ìš” íŒŒë¼ë¯¸í„° ì—†ìŒ) ìš”ì²­ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ stageì— í¬í•¨ì‹œí‚¤ì„¸ìš”.\n"
            "1. **Parallel Stage**: ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ê°€ í•„ìš”í•˜ì§€ ì•Šê³ , ì„œë¡œ ë…ë¦½ì ì¸ APIë“¤ì€ í•˜ë‚˜ì˜ Stage ì•ˆì— ë¦¬ìŠ¤íŠ¸ë¡œ ë„£ìœ¼ì„¸ìš”.\n"
            "2. **Sequential Stage**: ì´ì „ ë‹¨ê³„ì˜ ë°ì´í„°ê°€ ë°˜ë“œì‹œ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë‹¤ìŒ ìŠ¤í…Œì´ì§€ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.\n\n"


            f"[ì‚¬ìš© ê°€ëŠ¥í•œ API ëª…ì„¸ì„œ]\n{documents}"
        )
        plan_obj = await self.endpoint_planner_model.ainvoke(
            [SystemMessage(content=system_text), HumanMessage(content=question)],
            config={"callbacks": [self.debug_handler]}
        )
        plan_stages = [stage.model_dump() for stage in plan_obj.plan]
        all_endpoints = [step['endpoint'] for stage in plan_stages for step in stage['steps']]
        params_map = await self.get_params_info(all_endpoints)

        print("\nğŸ“‹ [High-Level Endpoint Plan]")
        for stage in plan_stages:
            print(f"   Stage {stage['stage_id']}")
            for step in stage['steps']:
                ep = step['endpoint']
                step['lookup_params'] = params_map.get(ep, "ì •ë³´ ì—†ìŒ")
                print(f"     - [{step['step_id']}] {ep} | Required Params: {step['lookup_params']}")
        return {"endpoint_plan": plan_stages, "current_stage_index": 0, "api_results": {}}

    async def param_planner_node(self, state: AgentState):
        print_state_debug("Param Planner", state)
        current_idx = state["current_stage_index"]
        endpoint_plan = state["endpoint_plan"]
        if not endpoint_plan or current_idx >= len(endpoint_plan):
            return {"executable_stage": {"steps": []}}
        
        current_stage_plan = endpoint_plan[current_idx]
        stage_id = current_stage_plan['stage_id']
        api_results = state.get("api_results", {})
        question = state["question"]
        print(f"\nğŸ§  [Planning Params for Stage {stage_id}]")
        
        prompt = (
            "ë‹¹ì‹ ì€ API íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” 'íŒŒë¼ë¯¸í„° í”Œë˜ë„ˆ'ì…ë‹ˆë‹¤.\n"
            "ì£¼ì–´ì§„ 'ë‹¤ìŒ ì‹¤í–‰ ê³„íš'ê³¼ 'ì´ì „ ë°ì´í„°'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ë²ˆì— ì‹¤í–‰í•  API í˜¸ì¶œ ëª©ë¡ì„ 'ì™„ì„±'í•˜ì„¸ìš”.\n\n"
            "## ì¤‘ìš” ê·œì¹™\n"
            "0. **íŒŒë¼ë¯¸í„° ê°’ ê·œì¹™**: ëª¨ë“  íŒŒë¼ë¯¸í„° ê°’ì€ 1ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. 0ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "1. **ë°˜ë³µ ì‹¤í–‰**: ë§Œì•½ ì´ì „ ë°ì´í„°ì— ë¦¬ìŠ¤íŠ¸(ì˜ˆ: ì¥ë¹„ 20ê°œ)ê°€ ìˆê³ , ë‹¤ìŒ ê³„íšì´ ê·¸ ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì— ëŒ€í•´ ì‹¤í–‰ë˜ì–´ì•¼ í•œë‹¤ë©´, ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  í•­ëª©ì— ëŒ€í•œ API í˜¸ì¶œì„ **ê°œë³„ì ìœ¼ë¡œ ëª¨ë‘ ìƒì„±**í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "2. **íŒŒë¼ë¯¸í„° ì±„ìš°ê¸°**: 'í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì •ë³´'ì™€ 'ì´ì „ ë°ì´í„°'ë¥¼ ë³´ê³  ê° API í˜¸ì¶œì— í•„ìš”í•œ íŒŒë¼ë¯¸í„° ê°’ì„ ì •í™•íˆ ì±„ì›Œë„£ìœ¼ì„¸ìš”.\n"
            "3. **ID ìƒì„±**: ê° `step_id`ëŠ” `{ì›ë˜ step_id}-instance-{n}` í˜•ì‹ìœ¼ë¡œ ê³ ìœ í•˜ê²Œ ë§Œë“œì„¸ìš”. (ì˜ˆ: '3-1-instance-1')\n\n"
            "--- ì…ë ¥ ì •ë³´ ---\n"
            f"### 1. ì „ì²´ ì‚¬ìš©ì ì§ˆë¬¸: {question}\n"
            f"### 2. ì´ì „ê¹Œì§€ì˜ API ì‹¤í–‰ ê²°ê³¼ (JSON): {json.dumps(api_results, ensure_ascii=False, indent=2)}\n"
            f"### 3. ì´ë²ˆì— ì‹¤í–‰í•  ìŠ¤í…Œì´ì§€ì˜ ì—”ë“œí¬ì¸íŠ¸ ê³„íš (JSON): {json.dumps(current_stage_plan, ensure_ascii=False, indent=2)}\n\n"
            "--- ì¶œë ¥ (Your turn) ---"
        )
        
        executable_stage_obj = await self.param_planner_model.ainvoke(
            [HumanMessage(content=prompt)],
            config={"callbacks": [self.debug_handler]}
        )
        executable_stage_dict = executable_stage_obj.model_dump()
        print("\nâœ¨ [Dynamically Generated Executable Stage]")
        for step in executable_stage_dict.get('steps', []):
            print(f"   - Step {step['step_id']}: {step['endpoint']} | Params: {step['params']}")
        return {"executable_stage": executable_stage_dict, "current_stage_index": state["current_stage_index"] + 1}

    async def _execute_single_step(self, step_info: Dict[str, Any]):
        result_data = await call_torus_api(step_info['endpoint'], step_info.get('params', {}))
        return {f"step_{step_info['step_id']}": {"request_endpoint": step_info['endpoint'], "request_params": step_info.get('params', {}), "response_data": result_data}}

    async def executor_node(self, state: AgentState):
        print_state_debug("Executor", state)
        steps_to_run = state.get("executable_stage", {}).get('steps', [])
        if not steps_to_run:
            print("   -> No steps to execute.")
            return {}
        
        api_results = state.get("api_results", {})
        print(f"\nğŸš€ [Executing Stage] - {len(steps_to_run)} parallel API calls")
        tasks = [self._execute_single_step(step) for step in steps_to_run]
        stage_results_list = await asyncio.gather(*tasks)
        
        for res_dict in stage_results_list:
            api_results.update(res_dict)
        return {"api_results": api_results}

    async def synthesizer_node(self, state: AgentState):
        print_state_debug("Synthesizer", state)
        final_prompt = (
            "ìˆ˜ì§‘ëœ API ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
            f"ì§ˆë¬¸: {state['question']}\n"
            f"ë°ì´í„°: {json.dumps(state['api_results'], ensure_ascii=False, indent=2)}"
        )
        response = await self.model.ainvoke(
            [HumanMessage(content=final_prompt)],
            config={"callbacks": [self.debug_handler]}
        )
        return {"final_answer": response.content}
    
    async def save_to_db_node(self, state: AgentState):
        print_state_debug("Save to DB", state)
        if state.get("from_db"):
            print("Skipping DB save because the plan was loaded from DB.")
            return {}
        
        question = state["question"]
        endpoint_plan = state["endpoint_plan"]
        
        db[question] = endpoint_plan
        db_permanent[question] = endpoint_plan
        save_db(DB_PATH, db)
        save_db(DB_PERMANENT_PATH, db_permanent)

        question_embedding = np.array([embedding_model.embed_query(question)], dtype='float32')
        index.add(question_embedding)
        save_faiss_index(index)
            
        print(f"ğŸ’¾ Saved question and plan to DB. Index size: {index.ntotal}")
        return {}

    # --- ì¡°ê±´ë¶€ ì—£ì§€ ---
    
    def decide_branch_after_rag(self, state: AgentState):
        if state.get("from_db"):
            print("--> RAG Success. Jumping to Param Planner.")
            return "param_planner"
        else:
            print("--> RAG Failed. Proceeding to Category Selection.")
            return "category_selector"

    def should_continue_planning(self, state: AgentState):
        if not state.get("executable_stage", {}).get("steps"):
            print("--> All stages complete. Proceeding to Synthesizer.")
            return "synthesizer"
        else:
            print("--> Steps generated. Proceeding to Executor.")
            return "executor"

    # --- ê·¸ë˜í”„ ë¹Œë“œ ---
    def build_graph(self):
        workflow = StateGraph(AgentState)
        
        workflow.add_node("vector_store", self.vector_store_node)
        workflow.add_node("category_selector", self.category_selection_node)
        workflow.add_node("endpoint_planner", self.endpoint_planner_node)
        workflow.add_node("param_planner", self.param_planner_node)
        workflow.add_node("executor", self.executor_node)
        workflow.add_node("synthesizer", self.synthesizer_node)
        workflow.add_node("save_to_db", self.save_to_db_node)
        
        workflow.set_entry_point("vector_store")
        
        workflow.add_conditional_edges(
            "vector_store",
            self.decide_branch_after_rag,
            {"param_planner": "param_planner", "category_selector": "category_selector"}
        )
        workflow.add_edge("category_selector", "endpoint_planner")
        workflow.add_edge("endpoint_planner", "param_planner")
        
        workflow.add_conditional_edges(
            "param_planner",
            self.should_continue_planning,
            {"executor": "executor", "synthesizer": "synthesizer"}
        )
        workflow.add_edge("executor", "param_planner")
        workflow.add_edge("synthesizer", "save_to_db")
        workflow.add_edge("save_to_db", END)
        
        return workflow.compile()

# -------------------------------------------------------------------------
# 4. Main ì‹¤í–‰
# -------------------------------------------------------------------------
async def main():
    # llm = ChatAnthropic(temperature=0, model="claude-sonnet-4-0")
    llm = ChatAnthropic(temperature=0, model="claude-haiku-4-5")

    bot = TorusDynamicAgent(model=llm)
    graph = bot.build_graph()

    # RAG ì‹¤íŒ¨ë¥¼ ìœ ë„í•  ìˆ˜ ìˆëŠ” ë³µí•©ì ì¸ ì§ˆë¬¸
    query = "ì§€ë©˜ìŠ¤ì— íƒ‘ì¬ëœ ncì˜ ëª¨ë¸ëª…ì´ ë­ì•¼"
    
    print(f"User Query: {query}")
    print("="*60)

    async for event in graph.astream({"question": query}):
        for node_name, values in event.items():
            if node_name == "synthesizer" and "final_answer" in values:
                print(f"\nâœ… [Final Answer]: {values['final_answer']}")

    print("\n" + "="*60)
    print("âœ… ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ")

if __name__ == "__main__":
    asyncio.run(main())
