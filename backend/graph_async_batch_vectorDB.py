import os
import json
import asyncio
import httpx
import logging
import faiss
import numpy as np
from typing import Annotated, List, Dict, Any, TypedDict, Union
from pathlib import Path
from dotenv import load_dotenv

from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from pydantic import BaseModel, Field
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_anthropic import ChatAnthropic
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEmbeddings

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
# ê¸°ë³¸ ë¡œê¹…ì€ ë„ê³  ì»¤ìŠ¤í…€ ì¶œë ¥ë§Œ ë´…ë‹ˆë‹¤
logging.basicConfig(level=logging.CRITICAL)

# -------------------------------------------------------------------------
# 0. VectorDB ê´€ë ¨ ì„¤ì •
# -------------------------------------------------------------------------
DB_PATH = Path("data/faiss_db.json")
DB_PERMANENT_PATH = Path("data/faiss_db_permanent.json")
INDEX_PATH = Path("data/faiss_index.bin")
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

def load_db(file_path):
    if DB_PATH.exists():
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_db(file_path, db):
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(db, f, ensure_ascii=False, indent=4)

def load_faiss_index():
    if INDEX_PATH.exists():
        return faiss.read_index(str(INDEX_PATH))
    # ì„ë² ë”© ì°¨ì›(768)ì— ë”°ë¼ ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±
    return faiss.IndexFlatL2(768)

def save_faiss_index(index):
    faiss.write_index(index, str(INDEX_PATH))

db = load_db(DB_PATH)
db_permanent = load_db(DB_PERMANENT_PATH)
index = load_faiss_index()

# -------------------------------------------------------------------------
# 1. ë””ë²„ê¹… ë° ë¡œê¹… ìœ í‹¸ë¦¬í‹° (í•µì‹¬ ì¶”ê°€ ì‚¬í•­)
# -------------------------------------------------------------------------

def load_json_file(file_path: Path) -> Dict:
    """JSON íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logging.critical(f"ì¹˜ëª…ì  ì˜¤ë¥˜: í•„ìˆ˜ ì„¤ì • íŒŒì¼({file_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        raise # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨
    except json.JSONDecodeError:
        logging.critical(f"ì¹˜ëª…ì  ì˜¤ë¥˜: ì„¤ì • íŒŒì¼({file_path})ì˜ JSON í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨

class DebugCallbackHandler(BaseCallbackHandler):

    def __init__(self, show_prompt: bool = True, show_token: bool = True):
        super().__init__()
        self.show_prompt = show_prompt # í”„ë¡¬í”„íŠ¸ ì¶œë ¥ ì—¬ë¶€
        self.show_token = show_token   # í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥ ì—¬ë¶€

    """LLM ì‹¤í–‰ ì‹œ í† í° ì‚¬ìš©ëŸ‰ê³¼ ë‚´ë¶€ ë™ì‘ì„ ìº¡ì²˜í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¸ë“¤ëŸ¬"""
    
    def on_chat_model_start(self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any) -> None:

        if not self.show_prompt:
            return
        
        print("\n" + "ğŸ”µ " * 10 + " [LLM INPUT PROMPT] " + "ğŸ”µ " * 10)
        for msg in messages[0]:
            content = str(msg.content)
            # ë§¤ë‰´ì–¼ì´ ë„ˆë¬´ ê¸¸ë©´ ì¶•ì•½í•´ì„œ ë³´ì—¬ì¤Œ
            if len(content) > 1500:
                display_content = content[:500] + "\n... (ì¤‘ëµ: ë§¤ë‰´ì–¼ ë³¸ë¬¸) ...\n" + content[-500:]
            else:
                display_content = content
            print(f"[{msg.type.upper()}]: {display_content}")
        print("ğŸ”µ " * 25 + "\n")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:

        if not self.show_token:
            return
        
        try:
            generation = response.generations[0][0]
            usage = {}
            if hasattr(generation, 'message') and hasattr(generation.message, 'response_metadata'):
                usage = generation.message.response_metadata.get('token_usage', {}) or generation.message.response_metadata.get('usage', {})
            elif hasattr(response, 'llm_output') and response.llm_output:
                usage = response.llm_output.get('token_usage', {})

            if usage:
                input_tokens = usage.get('input_tokens', 0) or usage.get('prompt_tokens', 0)
                output_tokens = usage.get('output_tokens', 0) or usage.get('completion_tokens', 0)
                total = usage.get('total_tokens', 0) or (input_tokens + output_tokens)
                print(f"ğŸ“Š [TOKEN USAGE] Input: {input_tokens} | Output: {output_tokens} | Total: {total}")
            else:
                print("ğŸ“Š [TOKEN USAGE] ë©”íƒ€ë°ì´í„° ì—†ìŒ")
        except Exception as e:
            print(f"ğŸ“Š [TOKEN USAGE] íŒŒì‹± ì‹¤íŒ¨: {e}")

def print_state_debug(node_name: str, state: Dict):
    """í˜„ì¬ Stateì˜ ìƒíƒœë¥¼ ê¹”ë”í•˜ê²Œ ì¶œë ¥"""
    print(f"\n" + "âš¡ " * 15)
    print(f"âš¡ [STATE DUMP] Node: {node_name}")
    print(f"âš¡ " * 15)

    keys_to_show = ["current_stage_index", "api_results", "final_answer"]

    if "endpoint_plan" in state and state["endpoint_plan"]:
        plan = state["endpoint_plan"]
        if plan:
            print(f"  - endpoint_plan: {len(plan)} stages loaded")
    
    if "executable_stage" in state and state["executable_stage"]:
        steps = state["executable_stage"].get('steps', [])
        print(f"  - executable_stage: {len(steps)} steps ready for execution")

    for k in keys_to_show:
        if k in state and state[k] is not None:
            val = state[k]
            if k == "api_results" and val:
                print(f"  - api_results: {list(val.keys())}")
            elif val is not None:
                print(f"  - {k}: {val}")
    print("-" * 60 + "\n")


# -------------------------------------------------------------------------
# 2. ì„¤ì • ë° ë°ì´í„° ëª¨ë¸
# -------------------------------------------------------------------------

MD_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'manual', 'torus.md')
TORUS_MD_CONTENT = ""
try:
    with open(MD_PATH, 'r', encoding='utf-8') as f:
        TORUS_MD_CONTENT = f.read()
except FileNotFoundError:
    print("âš ï¸ ë§¤ë‰´ì–¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

JSON_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'manual', 'uri_params.json')
PARAMS_JSON = load_json_file(JSON_PATH)

# --- Endpoint-Plannerì˜ ì¶œë ¥ ëª¨ë¸ ---
class ApiStep(BaseModel):
    step_id: str = Field(description="ë‹¨ê³„ ì‹ë³„ì (ì˜ˆ: '1-1', '1-2')")
    endpoint: str = Field(description="í˜¸ì¶œí•  API ì—”ë“œí¬ì¸íŠ¸")
    reasoning: str = Field(description="ì´ ë‹¨ê³„ì˜ ì‹¤í–‰ ëª©ì ")

class ParallelStage(BaseModel):
    stage_id: int = Field(description="ì‹¤í–‰ ìˆœì„œ (1ë¶€í„° ì‹œì‘)")
    steps: List[ApiStep] = Field(description="ì´ ë‹¨ê³„ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ì—”ë“œí¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸")

class EndpointExecutionPlan(BaseModel):
    plan: List[ParallelStage] = Field(description="ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë  ìŠ¤í…Œì´ì§€ ë¦¬ìŠ¤íŠ¸")

# --- Param-Plannerì˜ ì¶œë ¥ ëª¨ë¸ ---
class ExecutableStep(BaseModel):
    step_id: str = Field(description="ê³ ìœ  ì‹ë³„ì (ì˜ˆ: '3-1-instance-1')")
    endpoint: str = Field(description="í˜¸ì¶œí•  API ì—”ë“œí¬ì¸íŠ¸")
    params: Dict[str, Any] = Field(description="API í˜¸ì¶œì— ì‚¬ìš©í•  ì‹¤ì œ íŒŒë¼ë¯¸í„°")
    reasoning: str = Field(description="ì´ íŠ¹ì • API í˜¸ì¶œì„ ì‹¤í–‰í•˜ëŠ” ì´ìœ ")

class ExecutableStage(BaseModel):
    steps: List[ExecutableStep] = Field(description="ì´ë²ˆ ìŠ¤í…Œì´ì§€ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  API í˜¸ì¶œ ëª©ë¡")

# --- ë©”ì¸ Agent State ---
class AgentState(TypedDict):
    question: str
    endpoint_plan: List[Dict] 
    executable_stage: Dict 
    current_stage_index: int
    api_results: Dict
    final_answer: str
    from_db: bool # DBì—ì„œ ì™”ëŠ”ì§€ ì—¬ë¶€

# -------------------------------------------------------------------------
# 3. ë„êµ¬(Tool) ë° ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
# -------------------------------------------------------------------------

async def call_torus_api(endpoint: str, params: Dict[str, Any] = {}) -> Dict:
    base_url = "http://127.0.0.1:8000"
    url = f"{base_url}{endpoint}"
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, params=params, timeout=10.0)
            if response.status_code == 200:
                data = response.json()
                print(f"   ğŸ“¥ [API Response] Success from {url} | Params: {params}")
                return data
            else:
                print(f"   âš ï¸ [API Response] Failed from {url} | Params: {params} | Status: {response.status_code}")
                return {"error": f"Status {response.status_code}", "body": response.text}
    except Exception as e:
        print(f"   ğŸ”¥ [API Response] Exception from {url} | Params: {params} | Error: {e}")
        return {"error": str(e)}

class TorusDynamicAgent:
    def __init__(self, model):
        self.debug_handler = DebugCallbackHandler() # show_prompt=False
        self.model = model
        self.endpoint_planner_model = model.with_structured_output(EndpointExecutionPlan)
        self.param_planner_model = model.with_structured_output(ExecutableStage)
        self.PARAMS_JSON = PARAMS_JSON

    async def get_params_info(self, endpoint_list: List[str]):
        results = {}
        for endpoint in endpoint_list:
            endpoint_info = self.PARAMS_JSON.get(endpoint)
            results[endpoint] = endpoint_info.get("required_params") if endpoint_info else "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì—”ë“œí¬ì¸íŠ¸"
        return results
    
    async def vector_store_node(self, state: AgentState):
        print_state_debug("Vector Store", state)
        question = state["question"]
        
        # ë²¡í„° DBì— ì§ˆë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸
        if index.ntotal > 0:
            query_embedding = embedding_model.embed_query(question)
            query_embedding = np.array([query_embedding], dtype='float32')
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰
            distances, indices = index.search(query_embedding, 1)
            # ì„ê³„ê°’ì´ ë„ˆë¬´ ë‚®ì•„ì„œ ì™„ì „ ë™ì¼ ì§ˆë¬¸ì´ ì•„ë‹Œ ì´ìƒ ì‹¤íŒ¨í•¨.
            print(f"ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ : {distances[0][0]}")
            # íŠ¹ì • ì„ê³„ê°’ ì´í•˜ì¼ ê²½ìš° (ìœ ì‚¬ë„ê°€ ë†’ì„ ê²½ìš°)
            if distances[0][0] < 10: # ì„ê³„ê°’ 0.2ëŠ” ì¡°ì • ê°€ëŠ¥
                matched_question = list(db.keys())[indices[0][0]]
                print(f"ğŸ” Found similar question in DB: '{matched_question}' with distance {distances[0][0]}")
                return {
                    "endpoint_plan": db[matched_question],
                    "from_db": True,
                    "current_stage_index": 0, 
                    "api_results": {}
                }

        print("No similar question found in DB. Proceeding to Endpoint Planner.")
        return {"from_db": False}

    async def endpoint_planner_node(self, state: AgentState):
        print_state_debug("Endpoint Planner", state)
        question = state["question"]
        system_text = (
            "ë‹¹ì‹ ì€ TORUS API ë³‘ë ¬ ì²˜ë¦¬ ì„¤ê³„ìì…ë‹ˆë‹¤. ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•˜ì—¬ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ë˜, "
            "**ì„œë¡œ ì˜ì¡´ì„±ì´ ì—†ëŠ” API í˜¸ì¶œì€ ê°™ì€ ìŠ¤í…Œì´ì§€(Stage)ë¡œ ë¬¶ì–´ì•¼ í•©ë‹ˆë‹¤.**\n\n"
            "=== ì‘ì„± ê·œì¹™ ===\n"
            "1. **Parallel Stage**: ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ê°€ í•„ìš”í•˜ì§€ ì•Šê³ , ì„œë¡œ ë…ë¦½ì ì¸ APIë“¤ì€ í•˜ë‚˜ì˜ Stage ì•ˆì— ë¦¬ìŠ¤íŠ¸ë¡œ ë„£ìœ¼ì„¸ìš”.\n"
            "2. **Sequential Stage**: ì´ì „ ë‹¨ê³„ì˜ ë°ì´í„°ê°€ ë°˜ë“œì‹œ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë‹¤ìŒ ìŠ¤í…Œì´ì§€ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.\n\n"
            f"[API ë§¤ë‰´ì–¼]\n{TORUS_MD_CONTENT}"
        )
        plan_obj = await self.endpoint_planner_model.ainvoke(
            [SystemMessage(content=system_text), HumanMessage(content=question)],
            config={"callbacks": [self.debug_handler]}
        )
        plan_stages = [stage.model_dump() for stage in plan_obj.plan]
        all_endpoints = [step['endpoint'] for stage in plan_stages for step in stage['steps']]
        params_map = await self.get_params_info(all_endpoints)
        print("\nğŸ“‹ [High-Level Endpoint Plan]")
        for stage in plan_stages:
            print(f"   Stage {stage['stage_id']}")
            for step in stage['steps']:
                ep = step['endpoint']
                step['lookup_params'] = params_map.get(ep, "ì •ë³´ ì—†ìŒ")
                print(f"     - [{step['step_id']}] {ep} | Required Params: {step['lookup_params']}")
        return {"endpoint_plan": plan_stages, "current_stage_index": 0, "api_results": {}}

    async def param_planner_node(self, state: AgentState):
        print_state_debug("Param Planner", state)
        current_idx = state["current_stage_index"]
        endpoint_plan = state["endpoint_plan"]
        if current_idx >= len(endpoint_plan):
            return {"executable_stage": {"steps": []}}
        
        current_stage_plan = endpoint_plan[current_idx]
        stage_id = current_stage_plan['stage_id']
        api_results = state.get("api_results", {})
        question = state["question"]
        print(f"\nğŸ§  [Planning Params for Stage {stage_id}]")
        
        prompt = (
            "ë‹¹ì‹ ì€ API íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” 'íŒŒë¼ë¯¸í„° í”Œë˜ë„ˆ'ì…ë‹ˆë‹¤.\n"
            "ì£¼ì–´ì§„ 'ë‹¤ìŒ ì‹¤í–‰ ê³„íš'ê³¼ 'ì´ì „ ë°ì´í„°'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ë²ˆì— ì‹¤í–‰í•  API í˜¸ì¶œ ëª©ë¡ì„ 'ì™„ì„±'í•˜ì„¸ìš”.\n\n"
            "## ì¤‘ìš” ê·œì¹™\n"
            "0. **íŒŒë¼ë¯¸í„° ê°’ ê·œì¹™**: ëª¨ë“  íŒŒë¼ë¯¸í„° ê°’ì€ 1ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. 0ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "1. **ë°˜ë³µ ì‹¤í–‰**: ë§Œì•½ ì´ì „ ë°ì´í„°ì— ë¦¬ìŠ¤íŠ¸(ì˜ˆ: ì¥ë¹„ 20ê°œ)ê°€ ìˆê³ , ë‹¤ìŒ ê³„íšì´ ê·¸ ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì— ëŒ€í•´ ì‹¤í–‰ë˜ì–´ì•¼ í•œë‹¤ë©´, ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  í•­ëª©ì— ëŒ€í•œ API í˜¸ì¶œì„ **ê°œë³„ì ìœ¼ë¡œ ëª¨ë‘ ìƒì„±**í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "2. **íŒŒë¼ë¯¸í„° ì±„ìš°ê¸°**: 'í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì •ë³´'ì™€ 'ì´ì „ ë°ì´í„°'ë¥¼ ë³´ê³  ê° API í˜¸ì¶œì— í•„ìš”í•œ íŒŒë¼ë¯¸í„° ê°’ì„ ì •í™•íˆ ì±„ì›Œë„£ìœ¼ì„¸ìš”.\n"
            "3. **ID ìƒì„±**: ê° `step_id`ëŠ” `{ì›ë˜ step_id}-instance-{n}` í˜•ì‹ìœ¼ë¡œ ê³ ìœ í•˜ê²Œ ë§Œë“œì„¸ìš”. (ì˜ˆ: '3-1-instance-1')\n"
            "4. **JSON ì¶œë ¥**: ë°˜ë“œì‹œ `ExecutableStage` ëª¨ë¸ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
            "--- ì…ë ¥ ì •ë³´ ---\n"
            f"### 1. ì „ì²´ ì‚¬ìš©ì ì§ˆë¬¸: {question}\n"
            f"### 2. ì´ì „ê¹Œì§€ì˜ API ì‹¤í–‰ ê²°ê³¼ (JSON): {json.dumps(api_results, ensure_ascii=False, indent=2)}\n"
            f"### 3. ì´ë²ˆì— ì‹¤í–‰í•  ìŠ¤í…Œì´ì§€ì˜ ì—”ë“œí¬ì¸íŠ¸ ê³„íš (JSON): {json.dumps(current_stage_plan, ensure_ascii=False, indent=2)}\n\n"
            "--- ì¶œë ¥ (Your turn) ---"
        )
        
        executable_stage_obj = await self.param_planner_model.ainvoke(
            [HumanMessage(content=prompt)],
            config={"callbacks": [self.debug_handler]}
        )
        executable_stage_dict = executable_stage_obj.model_dump()
        print("\nâœ¨ [Dynamically Generated Executable Stage]")
        for step in executable_stage_dict.get('steps', []):
            print(f"   - Step {step['step_id']}: {step['endpoint']} | Params: {step['params']}")
        return {"executable_stage": executable_stage_dict, "current_stage_index": current_idx + 1}

    async def _execute_single_step(self, step_info: Dict[str, Any]):
        step_id = step_info['step_id']
        endpoint = step_info['endpoint']
        params = step_info.get('params', {})
        result_data = await call_torus_api(endpoint, params)
        return {f"step_{step_id}": {"request_endpoint": endpoint, "request_params": params, "response_data": result_data}}

    async def executor_node(self, state: AgentState):
        print_state_debug("Executor", state)
        executable_stage = state.get("executable_stage", {})
        steps_to_run = executable_stage.get('steps', [])
        if not steps_to_run:
            print("   -> No steps to execute.")
            return {}
        
        api_results = state.get("api_results", {})
        print(f"\nğŸš€ [Executing Stage] - {len(steps_to_run)} parallel API calls")
        tasks = [self._execute_single_step(step_info) for step_info in steps_to_run]
        stage_results_list = await asyncio.gather(*tasks)
        
        for res_dict in stage_results_list:
            api_results.update(res_dict)
        return {"api_results": api_results}

    async def synthesizer_node(self, state: AgentState):
        print_state_debug("Synthesizer", state)
        question = state["question"]
        results = state["api_results"]
        final_prompt = (
            "ìˆ˜ì§‘ëœ API ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
            f"ì§ˆë¬¸: {question}\n"
            f"ë°ì´í„°: {json.dumps(results, ensure_ascii=False, indent=2)}"
        )
        response = await self.model.ainvoke(
            [HumanMessage(content=final_prompt)],
            config={"callbacks": [self.debug_handler]}
        )
        return {"final_answer": response.content}
    
    async def save_to_db_node(self, state: AgentState):
        print_state_debug("Save to DB", state)

        question = state["question"]
        endpoint_plan = state["endpoint_plan"]
        
        db_permanent[question] = endpoint_plan
        db[question] = endpoint_plan
        # DBì—ì„œ ë¡œë“œí•œ ê²½ìš° ì˜êµ¬ ì €ì¥ë§Œ í•¨
        if state.get("from_db"):
            print("Skipping DB save because the plan was loaded from DB. Only save in permanent.json")
            save_db(DB_PERMANENT_PATH, db_permanent)
            return {}
        
        save_db(DB_PERMANENT_PATH, db_permanent)
        save_db(DB_PATH ,db)

        # Faiss ì¸ë±ìŠ¤ì— question ì„ë² ë”© ì¶”ê°€
        question_embedding = embedding_model.embed_query(question)
        question_embedding = np.array([question_embedding], dtype='float32')
        index.add(question_embedding)
        save_faiss_index(index)
            
        print(f"ğŸ’¾ Saved question and plan to DB. Index size: {index.ntotal}")
        return {}


    def should_continue(self, state: AgentState):
        executable_stage = state.get("executable_stage", {})
        if not executable_stage or not executable_stage.get("steps"):
            print("--> All stages complete. Proceeding to Synthesizer.")
            return "synthesizer"
        else:
            print("--> Steps generated. Proceeding to Executor.")
            return "executor"

    def after_vector_store(self, state: AgentState):
        if state.get("from_db"):
            return "param_planner"
        return "endpoint_planner"

    def build_graph(self):
        workflow = StateGraph(AgentState)
        workflow.add_node("vector_store", self.vector_store_node)
        workflow.add_node("endpoint_planner", self.endpoint_planner_node)
        workflow.add_node("param_planner", self.param_planner_node)
        workflow.add_node("executor", self.executor_node)
        workflow.add_node("synthesizer", self.synthesizer_node)
        workflow.add_node("save_to_db", self.save_to_db_node)
        
        workflow.set_entry_point("vector_store")

        workflow.add_conditional_edges(
            "vector_store",
            self.after_vector_store,
            {"param_planner": "param_planner", "endpoint_planner": "endpoint_planner"}
        )
        workflow.add_edge("endpoint_planner", "param_planner")
        workflow.add_conditional_edges(
            "param_planner",
            self.should_continue,
            {"executor": "executor", "synthesizer": "synthesizer"}
        )
        workflow.add_edge("executor", "param_planner")
        workflow.add_edge("synthesizer", "save_to_db")
        workflow.add_edge("save_to_db", END)
        
        return workflow.compile()

# -------------------------------------------------------------------------
# 4. Main ì‹¤í–‰
# -------------------------------------------------------------------------
async def main():
    # llm = ChatAnthropic(temperature=0, model="claude-sonnet-4-5")
    llm = ChatAnthropic(temperature=0, model="claude-haiku-4-5")

    bot = TorusDynamicAgent(model=llm)
    graph = bot.build_graph()
    query = "í™”ë‚™ê³¼ ì§€ë©˜ìŠ¤ê°€ í˜„ì¬ ë“±ë¡ëœ ìƒíƒœì¸ì§€ í™•ì¸í•˜ê³  ì¥ë¹„ì˜ ì±„ë„ì´ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ ì•Œë ¤ì¤˜"
    print(f"User Query: {query}")
    print("="*60)
    
# ê° ì¥ë¹„ì— íƒ‘ì¬ëœ ncì˜ ëª¨ë¸ëª…ì´ ë­ì•¼
# ì¥ë¹„ì˜ ì±„ë„ì´ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ ì•Œë ¤ì¤˜
# ì§€ë©˜ìŠ¤ ì¥ë¹„ì˜ ê° ì¶•ì— ê±¸ë¦¬ëŠ” ë¶€í•˜ë¥¼ ì•Œë ¤ì¤˜
# ì§€ë©˜ìŠ¤ì˜ zì¶•ì˜ ì†Œë¹„ ì „ë ¥ ì ì‚°ê°’ì„ ì•Œë ¤ì¤˜
# ì§€ë©˜ìŠ¤ì—ì„œ í˜„ì¬ê¹Œì§€ ê°€ê³µí•œ ì‘ì—…ë¬¼ ê°œìˆ˜, ì§€ê¸ˆê¹Œì§€ ê°€ê³µëœ ì‹œê°„ ì•Œë ¤ì¤˜
# ì§€ë©˜ìŠ¤ì˜ í˜„ì¬ ì‹¤í–‰ íŒŒì¼ ì •ë³´, ë©”ì¸ í”„ë¡œê·¸ë¨ íŒŒì¼ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜
# ê° ì¥ë¹„ì— ë°œìƒí•œ ì•ŒëŒ ì •ë³´ë¥¼ ì¡°íšŒí•´ì¤˜
# ì§€ë©˜ìŠ¤ì— ë“±ë¡ëœ ê³µêµ¬ë“¤ì˜ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€, ê³µêµ¬ ìƒíƒœë¥¼ ì¡°íšŒí•´ì¤˜

    async for event in graph.astream({"question": query}):
        for node_name, values in event.items():
            if node_name == "synthesizer" and "final_answer" in values:
                print(f"\nâœ… [Final Answer]: {values['final_answer']}")

    print("\n" + "="*60)
    print("âœ… ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ")

if __name__ == "__main__":

    asyncio.run(main())

