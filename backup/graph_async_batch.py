import os
import json
import asyncio
import httpx
import logging
from typing import Annotated, List, Dict, Any, TypedDict, Union
from pathlib import Path
from dotenv import load_dotenv

from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from pydantic import BaseModel, Field
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_anthropic import ChatAnthropic
from langgraph.graph import StateGraph, END

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
# ê¸°ë³¸ ë¡œê¹…ì€ ë„ê³  ì»¤ìŠ¤í…€ ì¶œë ¥ë§Œ ë´…ë‹ˆë‹¤
logging.basicConfig(level=logging.CRITICAL)

# -------------------------------------------------------------------------
# 1. ë””ë²„ê¹… ë° ë¡œê¹… ìœ í‹¸ë¦¬í‹° (í•µì‹¬ ì¶”ê°€ ì‚¬í•­)
# -------------------------------------------------------------------------

def load_json_file(file_path: Path) -> Dict:
    """JSON íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logging.critical(f"ì¹˜ëª…ì  ì˜¤ë¥˜: í•„ìˆ˜ ì„¤ì • íŒŒì¼({file_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        raise # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨
    except json.JSONDecodeError:
        logging.critical(f"ì¹˜ëª…ì  ì˜¤ë¥˜: ì„¤ì • íŒŒì¼({file_path})ì˜ JSON í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨

class DebugCallbackHandler(BaseCallbackHandler):
    """LLM ì‹¤í–‰ ì‹œ í† í° ì‚¬ìš©ëŸ‰ê³¼ ë‚´ë¶€ ë™ì‘ì„ ìº¡ì²˜í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¸ë“¤ëŸ¬"""
    
    def on_chat_model_start(self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any) -> None:
        print("\n" + "ğŸ”µ " * 10 + " [LLM INPUT PROMPT] " + "ğŸ”µ " * 10)
        for msg in messages[0]:
            content = str(msg.content)
            # ë§¤ë‰´ì–¼ì´ ë„ˆë¬´ ê¸¸ë©´ ì¶•ì•½í•´ì„œ ë³´ì—¬ì¤Œ
            if len(content) > 1500:
                display_content = content[:500] + "\n... (ì¤‘ëµ: ë§¤ë‰´ì–¼ ë³¸ë¬¸) ...\n" + content[-500:]
            else:
                display_content = content
            print(f"[{msg.type.upper()}]: {display_content}")
        print("ğŸ”µ " * 25 + "\n")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        try:
            generation = response.generations[0][0]
            usage = {}
            if hasattr(generation, 'message') and hasattr(generation.message, 'response_metadata'):
                usage = generation.message.response_metadata.get('token_usage', {}) or generation.message.response_metadata.get('usage', {})
            elif hasattr(response, 'llm_output') and response.llm_output:
                usage = response.llm_output.get('token_usage', {})

            if usage:
                input_tokens = usage.get('input_tokens', 0) or usage.get('prompt_tokens', 0)
                output_tokens = usage.get('output_tokens', 0) or usage.get('completion_tokens', 0)
                total = usage.get('total_tokens', 0) or (input_tokens + output_tokens)
                print(f"ğŸ“Š [TOKEN USAGE] Input: {input_tokens} | Output: {output_tokens} | Total: {total}")
            else:
                print("ğŸ“Š [TOKEN USAGE] ë©”íƒ€ë°ì´í„° ì—†ìŒ")
        except Exception as e:
            print(f"ğŸ“Š [TOKEN USAGE] íŒŒì‹± ì‹¤íŒ¨: {e}")

def print_state_debug(node_name: str, state: Dict):
    """í˜„ì¬ Stateì˜ ìƒíƒœë¥¼ ê¹”ë”í•˜ê²Œ ì¶œë ¥"""
    print(f"\n" + "âš¡ " * 15)
    print(f"âš¡ [STATE DUMP] Node: {node_name}")
    print(f"âš¡ " * 15)

    keys_to_show = ["current_stage_index", "api_results", "final_answer"]

    if "endpoint_plan" in state and state["endpoint_plan"]:
        plan = state["endpoint_plan"]
        if plan:
            print(f"  - endpoint_plan: {len(plan)} stages loaded")
    
    if "executable_stage" in state and state["executable_stage"]:
        steps = state["executable_stage"].get('steps', [])
        print(f"  - executable_stage: {len(steps)} steps ready for execution")

    for k in keys_to_show:
        if k in state and state[k] is not None:
            val = state[k]
            if k == "api_results" and val:
                print(f"  - api_results: {list(val.keys())}")
            elif val is not None:
                print(f"  - {k}: {val}")
    print("-" * 60 + "\n")


# -------------------------------------------------------------------------
# 2. ì„¤ì • ë° ë°ì´í„° ëª¨ë¸
# -------------------------------------------------------------------------

MD_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'manual', 'torus.md')
TORUS_MD_CONTENT = ""
try:
    with open(MD_PATH, 'r', encoding='utf-8') as f:
        TORUS_MD_CONTENT = f.read()
except FileNotFoundError:
    print("âš ï¸ ë§¤ë‰´ì–¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

JSON_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'manual', 'uri_params.json')
PARAMS_JSON = load_json_file(JSON_PATH)

# --- Endpoint-Plannerì˜ ì¶œë ¥ ëª¨ë¸ ---
class ApiStep(BaseModel):
    step_id: str = Field(description="ë‹¨ê³„ ì‹ë³„ì (ì˜ˆ: '1-1', '1-2')")
    endpoint: str = Field(description="í˜¸ì¶œí•  API ì—”ë“œí¬ì¸íŠ¸")
    reasoning: str = Field(description="ì´ ë‹¨ê³„ì˜ ì‹¤í–‰ ëª©ì ")
    required_params_desc: str = Field(description="í•„ìš”í•œ íŒŒë¼ë¯¸í„° íŒíŠ¸")

class ParallelStage(BaseModel):
    stage_id: int = Field(description="ì‹¤í–‰ ìˆœì„œ (1ë¶€í„° ì‹œì‘)")
    steps: List[ApiStep] = Field(description="ì´ ë‹¨ê³„ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ì—”ë“œí¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸")

class EndpointExecutionPlan(BaseModel):
    plan: List[ParallelStage] = Field(description="ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë  ìŠ¤í…Œì´ì§€ ë¦¬ìŠ¤íŠ¸")

# --- Param-Plannerì˜ ì¶œë ¥ ëª¨ë¸ ---
class ExecutableStep(BaseModel):
    step_id: str = Field(description="ê³ ìœ  ì‹ë³„ì (ì˜ˆ: '3-1-instance-1')")
    endpoint: str = Field(description="í˜¸ì¶œí•  API ì—”ë“œí¬ì¸íŠ¸")
    params: Dict[str, Any] = Field(description="API í˜¸ì¶œì— ì‚¬ìš©í•  ì‹¤ì œ íŒŒë¼ë¯¸í„°")
    reasoning: str = Field(description="ì´ íŠ¹ì • API í˜¸ì¶œì„ ì‹¤í–‰í•˜ëŠ” ì´ìœ ")

class ExecutableStage(BaseModel):
    steps: List[ExecutableStep] = Field(description="ì´ë²ˆ ìŠ¤í…Œì´ì§€ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  API í˜¸ì¶œ ëª©ë¡")

# --- ë©”ì¸ Agent State ---
class AgentState(TypedDict):
    question: str
    endpoint_plan: List[Dict] 
    executable_stage: Dict 
    current_stage_index: int
    api_results: Dict
    final_answer: str

# -------------------------------------------------------------------------
# 3. ë„êµ¬(Tool) ë° ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
# -------------------------------------------------------------------------

async def call_torus_api(endpoint: str, params: Dict[str, Any] = {}) -> Dict:
    base_url = "http://127.0.0.1:8000"
    url = f"{base_url}{endpoint}"
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, params=params, timeout=10.0)
            if response.status_code == 200:
                data = response.json()
                print(f"   ğŸ“¥ [API Response] Success from {url} | Params: {params}")
                return data
            else:
                print(f"   âš ï¸ [API Response] Failed from {url} | Params: {params} | Status: {response.status_code}")
                return {"error": f"Status {response.status_code}", "body": response.text}
    except Exception as e:
        print(f"   ğŸ”¥ [API Response] Exception from {url} | Params: {params} | Error: {e}")
        return {"error": str(e)}

class TorusDynamicAgent:
    def __init__(self, model):
        self.debug_handler = DebugCallbackHandler()
        self.model = model
        self.endpoint_planner_model = model.with_structured_output(EndpointExecutionPlan)
        self.param_planner_model = model.with_structured_output(ExecutableStage)
        self.PARAMS_JSON = PARAMS_JSON

    async def get_params_info(self, endpoint_list: List[str]):
        results = {}
        for endpoint in endpoint_list:
            endpoint_info = self.PARAMS_JSON.get(endpoint)
            results[endpoint] = endpoint_info.get("required_params") if endpoint_info else "ë§¤ë‰´ì–¼ ì°¸ì¡° í•„ìš”"
        return results

    async def endpoint_planner_node(self, state: AgentState):
        print_state_debug("Endpoint Planner", state)
        question = state["question"]
        system_text = (
            "ë‹¹ì‹ ì€ TORUS API ë³‘ë ¬ ì²˜ë¦¬ ì„¤ê³„ìì…ë‹ˆë‹¤. ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•˜ì—¬ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ë˜, "
            "**ì„œë¡œ ì˜ì¡´ì„±ì´ ì—†ëŠ” API í˜¸ì¶œì€ ê°™ì€ ìŠ¤í…Œì´ì§€(Stage)ë¡œ ë¬¶ì–´ì•¼ í•©ë‹ˆë‹¤.**\n\n"
            "=== ì‘ì„± ê·œì¹™ ===\n"
            "1. **Parallel Stage**: ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ê°€ í•„ìš”í•˜ì§€ ì•Šê³ , ì„œë¡œ ë…ë¦½ì ì¸ APIë“¤ì€ í•˜ë‚˜ì˜ Stage ì•ˆì— ë¦¬ìŠ¤íŠ¸ë¡œ ë„£ìœ¼ì„¸ìš”.\n"
            "2. **Sequential Stage**: ì´ì „ ë‹¨ê³„ì˜ ë°ì´í„°ê°€ ë°˜ë“œì‹œ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë‹¤ìŒ ìŠ¤í…Œì´ì§€ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.\n\n"
            f"[API ë§¤ë‰´ì–¼]\n{TORUS_MD_CONTENT}"
        )
        plan_obj = await self.endpoint_planner_model.ainvoke(
            [SystemMessage(content=system_text), HumanMessage(content=question)],
            config={"callbacks": [self.debug_handler]}
        )
        plan_stages = [stage.model_dump() for stage in plan_obj.plan]
        all_endpoints = [step['endpoint'] for stage in plan_stages for step in stage['steps']]
        params_map = await self.get_params_info(all_endpoints)
        print("\nğŸ“‹ [High-Level Endpoint Plan]")
        for stage in plan_stages:
            print(f"   Stage {stage['stage_id']}")
            for step in stage['steps']:
                ep = step['endpoint']
                step['lookup_params'] = params_map.get(ep, "ì •ë³´ ì—†ìŒ")
                print(f"     - [{step['step_id']}] {ep} | Required Params: {step['lookup_params']}")
        return {"endpoint_plan": plan_stages, "current_stage_index": 0, "api_results": {}}

    async def param_planner_node(self, state: AgentState):
        print_state_debug("Param Planner", state)
        current_idx = state["current_stage_index"]
        endpoint_plan = state["endpoint_plan"]
        if current_idx >= len(endpoint_plan):
            return {"executable_stage": {"steps": []}}
        
        current_stage_plan = endpoint_plan[current_idx]
        stage_id = current_stage_plan['stage_id']
        api_results = state.get("api_results", {})
        question = state["question"]
        print(f"\nğŸ§  [Planning Params for Stage {stage_id}]")
        
        prompt = (
            "ë‹¹ì‹ ì€ API íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” 'íŒŒë¼ë¯¸í„° í”Œë˜ë„ˆ'ì…ë‹ˆë‹¤.\n"
            "ì£¼ì–´ì§„ 'ë‹¤ìŒ ì‹¤í–‰ ê³„íš'ê³¼ 'ì´ì „ ë°ì´í„°'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ë²ˆì— ì‹¤í–‰í•  API í˜¸ì¶œ ëª©ë¡ì„ 'ì™„ì„±'í•˜ì„¸ìš”.\n\n"
            "## ì¤‘ìš” ê·œì¹™\n"
            "1. **ë°˜ë³µ ì‹¤í–‰**: ë§Œì•½ ì´ì „ ë°ì´í„°ì— ë¦¬ìŠ¤íŠ¸(ì˜ˆ: ì¥ë¹„ 20ê°œ)ê°€ ìˆê³ , ë‹¤ìŒ ê³„íšì´ ê·¸ ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì— ëŒ€í•´ ì‹¤í–‰ë˜ì–´ì•¼ í•œë‹¤ë©´, ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  í•­ëª©ì— ëŒ€í•œ API í˜¸ì¶œì„ **ê°œë³„ì ìœ¼ë¡œ ëª¨ë‘ ìƒì„±**í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "2. **íŒŒë¼ë¯¸í„° ì±„ìš°ê¸°**: 'í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì •ë³´'ì™€ 'ì´ì „ ë°ì´í„°'ë¥¼ ë³´ê³  ê° API í˜¸ì¶œì— í•„ìš”í•œ íŒŒë¼ë¯¸í„° ê°’ì„ ì •í™•íˆ ì±„ì›Œë„£ìœ¼ì„¸ìš”.\n"
            "3. **ID ìƒì„±**: ê° `step_id`ëŠ” `{ì›ë˜ step_id}-instance-{n}` í˜•ì‹ìœ¼ë¡œ ê³ ìœ í•˜ê²Œ ë§Œë“œì„¸ìš”. (ì˜ˆ: '3-1-instance-1')\n"
            "4. **JSON ì¶œë ¥**: ë°˜ë“œì‹œ `ExecutableStage` ëª¨ë¸ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
            "--- ì…ë ¥ ì •ë³´ ---\n"
            f"### 1. ì „ì²´ ì‚¬ìš©ì ì§ˆë¬¸: {question}\n"
            f"### 2. ì´ì „ê¹Œì§€ì˜ API ì‹¤í–‰ ê²°ê³¼ (JSON): {json.dumps(api_results, ensure_ascii=False, indent=2)}\n"
            f"### 3. ì´ë²ˆì— ì‹¤í–‰í•  ìŠ¤í…Œì´ì§€ì˜ ì—”ë“œí¬ì¸íŠ¸ ê³„íš (JSON): {json.dumps(current_stage_plan, ensure_ascii=False, indent=2)}\n\n"
            "--- ì¶œë ¥ (Your turn) ---"
        )
        
        executable_stage_obj = await self.param_planner_model.ainvoke(
            [HumanMessage(content=prompt)],
            config={"callbacks": [self.debug_handler]}
        )
        executable_stage_dict = executable_stage_obj.model_dump()
        print("\nâœ¨ [Dynamically Generated Executable Stage]")
        for step in executable_stage_dict.get('steps', []):
            print(f"   - Step {step['step_id']}: {step['endpoint']} | Params: {step['params']}")
        return {"executable_stage": executable_stage_dict, "current_stage_index": current_idx + 1}

    async def _execute_single_step(self, step_info: Dict[str, Any]):
        step_id = step_info['step_id']
        endpoint = step_info['endpoint']
        params = step_info.get('params', {})
        result_data = await call_torus_api(endpoint, params)
        return {f"step_{step_id}": {"request_endpoint": endpoint, "request_params": params, "response_data": result_data}}

    async def executor_node(self, state: AgentState):
        print_state_debug("Executor", state)
        executable_stage = state.get("executable_stage", {})
        steps_to_run = executable_stage.get('steps', [])
        if not steps_to_run:
            print("   -> No steps to execute.")
            return {}
        
        api_results = state.get("api_results", {})
        print(f"\nğŸš€ [Executing Stage] - {len(steps_to_run)} parallel API calls")
        tasks = [self._execute_single_step(step_info) for step_info in steps_to_run]
        stage_results_list = await asyncio.gather(*tasks)
        
        for res_dict in stage_results_list:
            api_results.update(res_dict)
        return {"api_results": api_results}

    async def synthesizer_node(self, state: AgentState):
        print_state_debug("Synthesizer", state)
        question = state["question"]
        results = state["api_results"]
        final_prompt = (
            "ìˆ˜ì§‘ëœ API ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
            f"ì§ˆë¬¸: {question}\n"
            f"ë°ì´í„°: {json.dumps(results, ensure_ascii=False, indent=2)}"
        )
        response = await self.model.ainvoke(
            [HumanMessage(content=final_prompt)],
            config={"callbacks": [self.debug_handler]}
        )
        return {"final_answer": response.content}

    def should_continue(self, state: AgentState):
        executable_stage = state.get("executable_stage", {})
        if not executable_stage or not executable_stage.get("steps"):
            print("--> All stages complete. Proceeding to Synthesizer.")
            return "synthesizer"
        else:
            print("--> Steps generated. Proceeding to Executor.")
            return "executor"

    def build_graph(self):
        workflow = StateGraph(AgentState)
        workflow.add_node("endpoint_planner", self.endpoint_planner_node)
        workflow.add_node("param_planner", self.param_planner_node)
        workflow.add_node("executor", self.executor_node)
        workflow.add_node("synthesizer", self.synthesizer_node)
        
        workflow.set_entry_point("endpoint_planner")
        workflow.add_edge("endpoint_planner", "param_planner")
        workflow.add_conditional_edges(
            "param_planner",
            self.should_continue,
            {"executor": "executor", "synthesizer": "synthesizer"}
        )
        workflow.add_edge("executor", "param_planner")
        workflow.add_edge("synthesizer", END)
        
        return workflow.compile()

# -------------------------------------------------------------------------
# 4. Main ì‹¤í–‰
# -------------------------------------------------------------------------
async def main():
    llm = ChatAnthropic(temperature=0, model="claude-sonnet-4-20250514")

    bot = TorusDynamicAgent(model=llm)
    graph = bot.build_graph()
    
    query = "ì§€ë©˜ìŠ¤ì— ë“±ë¡ëœ 1ë²ˆ ê³µêµ¬ì˜ ìˆ˜ëª… ì •ë³´ë¥¼ ì•Œë ¤ì¤˜"
    print(f"User Query: {query}")
    print("="*60)
    
    async for event in graph.astream({"question": query}):
        for node_name, values in event.items():
            if node_name == "synthesizer" and "final_answer" in values:
                print(f"\nâœ… [Final Answer]: {values['final_answer']}")

    print("\n" + "="*60)
    print("âœ… ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ")

if __name__ == "__main__":
    asyncio.run(main())
